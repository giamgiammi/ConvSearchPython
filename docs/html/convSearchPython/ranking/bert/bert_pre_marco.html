<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>convSearchPython.ranking.bert.bert_pre_marco API documentation</title>
<meta name="description" content="Load pre-trained BERT (dl4marco-bert)" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML" integrity="sha256-kZafAc6mZvK3W3v1pHOcUix30OHQN6pU/NO2oFkqZVw=" crossorigin></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>convSearchPython.ranking.bert.bert_pre_marco</code></h1>
</header>
<section id="section-intro">
<p>Load pre-trained BERT (dl4marco-bert)</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;Load pre-trained BERT (dl4marco-bert)&#34;&#34;&#34;
import gc
import logging
from abc import abstractmethod
from os import environ

import numpy as np
import pandas as pd
import torch
import torch.cuda as cuda
from torch.nn import DataParallel
from transformers import BertForSequenceClassification, BertTokenizerFast

CACHE = {&#39;batch_size&#39;: 80}


# torch.use_deterministic_algorithms(True)

if &#39;NO_PRELOAD&#39; not in environ:
    # make sure required data is downloaded when this module is loaded
    BertForSequenceClassification.from_pretrained(&#39;castorini/monobert-large-msmarco-finetune-only&#39;)


class DL4MBertClassifier:
    @abstractmethod
    def log_prob_df(self, *args, **kwargs) -&gt; pd.DataFrame:
        &#34;&#34;&#34;Compute the log-likelihood between queries and doc in a dataset&#34;&#34;&#34;
        raise NotImplementedError()


class CpuDL4MBertClassifier(DL4MBertClassifier):
    &#34;&#34;&#34;Classifier for the dl4marco-bert model pre-trained on MSMARCO&#34;&#34;&#34;

    def __init__(self):
        self.model = BertForSequenceClassification.from_pretrained(&#39;castorini/monobert-large-msmarco-finetune-only&#39;) \
            .eval()
        self.tokenizer = BertTokenizerFast.from_pretrained(&#39;castorini/monobert-large-msmarco-finetune-only&#39;)

    def log_prob(self, query: str, doc: str) -&gt; float:
        &#34;&#34;&#34;Compute the log-likelihood probability between a query and a document

        :param query: the query text
        :param doc: the document text
        :return: a float in [0,1] representing the log-likelihood probability&#34;&#34;&#34;
        encoding = self.tokenizer(query, doc, return_tensors=&#34;pt&#34;, truncation=&#39;only_second&#39;)
        logits = self.model(**encoding).logits
        sft = torch.softmax(logits, dim=1)
        return (- torch.log(sft[0][1])).item()

    def log_prob_df(self, data: pd.DataFrame, inplace=False) -&gt; pd.DataFrame:
        &#34;&#34;&#34;Compute the log-likelihood between queries and doc in a dataset.

        :param data: input df
        :param inplace: if modify data or create a copy
        :return: df with &#39;log_prob&#39; column&#34;&#34;&#34;
        if not inplace:
            data = data.copy()
        data[&#39;log_prob&#39;] = np.vectorize(self.log_prob)(data[&#39;query&#39;], data[&#39;text&#39;])
        return data


class GpuDL4MBertClassifier(DL4MBertClassifier):
    &#34;&#34;&#34;Classifier for the dl4marco-bert model pre-trained on MSMARCO&#34;&#34;&#34;

    def __init__(self):
        model = BertForSequenceClassification.from_pretrained(&#39;castorini/monobert-large-msmarco-finetune-only&#39;)
        model = model.eval()
        model = DataParallel(model.to(&#39;cuda:0&#39;))
        self.model = model
        self.tokenizer = BertTokenizerFast.from_pretrained(&#39;castorini/monobert-large-msmarco-finetune-only&#39;)
        self.__logger = logging.getLogger(self.__class__.__name__)
        self.__logger.info(&#39;Using cuda: %s&#39;, cuda.get_device_name(cuda.current_device()))

    def log_prob(self, query: str, doc: str) -&gt; float:
        &#34;&#34;&#34;Compute the log-likelihood probability between a query and a document

        :param query: the query text
        :param doc: the document text
        :return: a float in [0,1] representing the log-likelihood probability&#34;&#34;&#34;
        encoding = self.tokenizer(query, doc, return_tensors=&#34;pt&#34;, truncation=&#39;only_second&#39;)
        logits = self.model(**encoding).logits
        sft = torch.softmax(logits, dim=1)
        lg = - torch.log(sft[0][1])
        return lg.item()

    def _log_prob_batch(self, batch_a, batch_b):
        try:
            # return True, np.fromiter(
            #     ((-torch.log(s[1])).item() for s in torch.softmax(
            #         self.model(**(
            #             self.tokenizer(batch_a, batch_b, return_tensors=&#34;pt&#34;, truncation=&#39;only_second&#39;, padding=True)
            #         )).logits, dim=1
            #     )), float, len(batch_a)
            # )
            # return True, np.fromiter(
            #     (ls[1].item() for ls in -torch.log_softmax(
            #         self.model(**(
            #             self.tokenizer(batch_a, batch_b, return_tensors=&#34;pt&#34;, truncation=&#39;only_second&#39;, padding=True)
            #         )).logits, dim=1
            #     )), float, len(batch_a)
            # )
            return True, np.fromiter(
                (item[1] for item in (-torch.log_softmax(
                    self.model(**(
                        self.tokenizer(batch_a, batch_b, return_tensors=&#34;pt&#34;, truncation=&#39;only_second&#39;, padding=True)
                    )).logits, dim=1)
                ).detach().cpu().numpy()), float, len(batch_a)
            )
        except RuntimeError as ex:  # Cuda Out Of Memory
            self.__logger.warning(ex)
            return False, str(ex)

    def _log_prob_loop(self, batch_size: int, queries: list, size_reduce: int, texts: list):
        size = len(queries)
        count = 0
        results = []
        while count &lt; size:
            ok, res = self._log_prob_batch(
                queries[count:count + batch_size],
                texts[count:count + batch_size]
            )
            if not ok:
                if batch_size == 1:
                    self.__logger.error(&#39;Runtime error during computation: %s&#39;, res)
                    raise Exception()
                new_size = batch_size - size_reduce
                new_size = new_size if new_size &gt;= 1 else 1
                self.__logger.warning(&#39;Trying to reduce batch size from %d to %d&#39;, batch_size, new_size)
                batch_size = new_size
                gc.collect()  # force garbage collection
                continue
            results.append(res)
            count = count + batch_size
        CACHE[&#39;batch_size&#39;] = batch_size
        return np.concatenate(results, dtype=float)

    def log_prob_df(self, data: pd.DataFrame, inplace=False, batch_size: int = None, size_reduce=5) -&gt; pd.DataFrame:
        &#34;&#34;&#34;Compute the log-likelihood between queries and doc in a dataset.

        :param data: input df
        :param inplace: if modify data or create a copy
        :param batch_size: number of rows to compute at time
        :param size_reduce: how much reduce batch_size if computation fail for OOM
        :return: df with &#39;log_prob&#39; column&#34;&#34;&#34;
        if not inplace:
            data = data.copy()

        if batch_size is None:
            batch_size = CACHE[&#39;batch_size&#39;]

        queries = list(data[&#39;query&#39;])
        texts = list(data[&#39;text&#39;])
        log_col = self._log_prob_loop(batch_size, queries, size_reduce, texts)

        data[&#39;log_prob&#39;] = log_col
        return data


if __name__ == &#39;__main__&#39;:
    _classifier = GpuDL4MBertClassifier()
    _query = &#34;Good Morning at everyone!&#34;
    _docs = [&#39;Good day&#39;, &#39;Bad week&#39;, &#39;Everyone listen!&#39;, &#39;A good morning for everyone.&#39;, _query]
    _docs.append(&#39; &#39;.join(&#39;HI&#39; for _ in range(513)))

    print(f&#39;Likelihood with query &#34;{_query}&#34;&#39;)
    _df = pd.DataFrame()
    _df[&#39;text&#39;] = _docs
    _df[&#39;query&#39;] = [_query for _ in range(len(_docs))]
    _df2 = _classifier.log_prob_df(_df, batch_size=5)
    print(_df2)
    print(_df2.iloc[0][&#39;log_prob&#39;])
    print(type(_df2.iloc[0][&#39;log_prob&#39;]))
    print(type(_df2.iloc[0][&#39;query&#39;]))

    _df3 = CpuDL4MBertClassifier().log_prob_df(_df)
    print(_df3)

    print(&#39;END&#39;)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="convSearchPython.ranking.bert.bert_pre_marco.DL4MBertClassifier"><code class="flex name class">
<span>class <span class="ident">DL4MBertClassifier</span></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DL4MBertClassifier:
    @abstractmethod
    def log_prob_df(self, *args, **kwargs) -&gt; pd.DataFrame:
        &#34;&#34;&#34;Compute the log-likelihood between queries and doc in a dataset&#34;&#34;&#34;
        raise NotImplementedError()</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="convSearchPython.ranking.bert.bert_pre_marco.CpuDL4MBertClassifier" href="#convSearchPython.ranking.bert.bert_pre_marco.CpuDL4MBertClassifier">CpuDL4MBertClassifier</a></li>
<li><a title="convSearchPython.ranking.bert.bert_pre_marco.GpuDL4MBertClassifier" href="#convSearchPython.ranking.bert.bert_pre_marco.GpuDL4MBertClassifier">GpuDL4MBertClassifier</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="convSearchPython.ranking.bert.bert_pre_marco.DL4MBertClassifier.log_prob_df"><code class="name flex">
<span>def <span class="ident">log_prob_df</span></span>(<span>self, *args, **kwargs) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the log-likelihood between queries and doc in a dataset</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def log_prob_df(self, *args, **kwargs) -&gt; pd.DataFrame:
    &#34;&#34;&#34;Compute the log-likelihood between queries and doc in a dataset&#34;&#34;&#34;
    raise NotImplementedError()</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="convSearchPython.ranking.bert.bert_pre_marco.CpuDL4MBertClassifier"><code class="flex name class">
<span>class <span class="ident">CpuDL4MBertClassifier</span></span>
</code></dt>
<dd>
<div class="desc"><p>Classifier for the dl4marco-bert model pre-trained on MSMARCO</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class CpuDL4MBertClassifier(DL4MBertClassifier):
    &#34;&#34;&#34;Classifier for the dl4marco-bert model pre-trained on MSMARCO&#34;&#34;&#34;

    def __init__(self):
        self.model = BertForSequenceClassification.from_pretrained(&#39;castorini/monobert-large-msmarco-finetune-only&#39;) \
            .eval()
        self.tokenizer = BertTokenizerFast.from_pretrained(&#39;castorini/monobert-large-msmarco-finetune-only&#39;)

    def log_prob(self, query: str, doc: str) -&gt; float:
        &#34;&#34;&#34;Compute the log-likelihood probability between a query and a document

        :param query: the query text
        :param doc: the document text
        :return: a float in [0,1] representing the log-likelihood probability&#34;&#34;&#34;
        encoding = self.tokenizer(query, doc, return_tensors=&#34;pt&#34;, truncation=&#39;only_second&#39;)
        logits = self.model(**encoding).logits
        sft = torch.softmax(logits, dim=1)
        return (- torch.log(sft[0][1])).item()

    def log_prob_df(self, data: pd.DataFrame, inplace=False) -&gt; pd.DataFrame:
        &#34;&#34;&#34;Compute the log-likelihood between queries and doc in a dataset.

        :param data: input df
        :param inplace: if modify data or create a copy
        :return: df with &#39;log_prob&#39; column&#34;&#34;&#34;
        if not inplace:
            data = data.copy()
        data[&#39;log_prob&#39;] = np.vectorize(self.log_prob)(data[&#39;query&#39;], data[&#39;text&#39;])
        return data</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="convSearchPython.ranking.bert.bert_pre_marco.DL4MBertClassifier" href="#convSearchPython.ranking.bert.bert_pre_marco.DL4MBertClassifier">DL4MBertClassifier</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="convSearchPython.ranking.bert.bert_pre_marco.CpuDL4MBertClassifier.log_prob"><code class="name flex">
<span>def <span class="ident">log_prob</span></span>(<span>self, query: str, doc: str) ‑> float</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the log-likelihood probability between a query and a document</p>
<p>:param query: the query text
:param doc: the document text
:return: a float in [0,1] representing the log-likelihood probability</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def log_prob(self, query: str, doc: str) -&gt; float:
    &#34;&#34;&#34;Compute the log-likelihood probability between a query and a document

    :param query: the query text
    :param doc: the document text
    :return: a float in [0,1] representing the log-likelihood probability&#34;&#34;&#34;
    encoding = self.tokenizer(query, doc, return_tensors=&#34;pt&#34;, truncation=&#39;only_second&#39;)
    logits = self.model(**encoding).logits
    sft = torch.softmax(logits, dim=1)
    return (- torch.log(sft[0][1])).item()</code></pre>
</details>
</dd>
<dt id="convSearchPython.ranking.bert.bert_pre_marco.CpuDL4MBertClassifier.log_prob_df"><code class="name flex">
<span>def <span class="ident">log_prob_df</span></span>(<span>self, data: pandas.core.frame.DataFrame, inplace=False) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the log-likelihood between queries and doc in a dataset.</p>
<p>:param data: input df
:param inplace: if modify data or create a copy
:return: df with 'log_prob' column</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def log_prob_df(self, data: pd.DataFrame, inplace=False) -&gt; pd.DataFrame:
    &#34;&#34;&#34;Compute the log-likelihood between queries and doc in a dataset.

    :param data: input df
    :param inplace: if modify data or create a copy
    :return: df with &#39;log_prob&#39; column&#34;&#34;&#34;
    if not inplace:
        data = data.copy()
    data[&#39;log_prob&#39;] = np.vectorize(self.log_prob)(data[&#39;query&#39;], data[&#39;text&#39;])
    return data</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="convSearchPython.ranking.bert.bert_pre_marco.GpuDL4MBertClassifier"><code class="flex name class">
<span>class <span class="ident">GpuDL4MBertClassifier</span></span>
</code></dt>
<dd>
<div class="desc"><p>Classifier for the dl4marco-bert model pre-trained on MSMARCO</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class GpuDL4MBertClassifier(DL4MBertClassifier):
    &#34;&#34;&#34;Classifier for the dl4marco-bert model pre-trained on MSMARCO&#34;&#34;&#34;

    def __init__(self):
        model = BertForSequenceClassification.from_pretrained(&#39;castorini/monobert-large-msmarco-finetune-only&#39;)
        model = model.eval()
        model = DataParallel(model.to(&#39;cuda:0&#39;))
        self.model = model
        self.tokenizer = BertTokenizerFast.from_pretrained(&#39;castorini/monobert-large-msmarco-finetune-only&#39;)
        self.__logger = logging.getLogger(self.__class__.__name__)
        self.__logger.info(&#39;Using cuda: %s&#39;, cuda.get_device_name(cuda.current_device()))

    def log_prob(self, query: str, doc: str) -&gt; float:
        &#34;&#34;&#34;Compute the log-likelihood probability between a query and a document

        :param query: the query text
        :param doc: the document text
        :return: a float in [0,1] representing the log-likelihood probability&#34;&#34;&#34;
        encoding = self.tokenizer(query, doc, return_tensors=&#34;pt&#34;, truncation=&#39;only_second&#39;)
        logits = self.model(**encoding).logits
        sft = torch.softmax(logits, dim=1)
        lg = - torch.log(sft[0][1])
        return lg.item()

    def _log_prob_batch(self, batch_a, batch_b):
        try:
            # return True, np.fromiter(
            #     ((-torch.log(s[1])).item() for s in torch.softmax(
            #         self.model(**(
            #             self.tokenizer(batch_a, batch_b, return_tensors=&#34;pt&#34;, truncation=&#39;only_second&#39;, padding=True)
            #         )).logits, dim=1
            #     )), float, len(batch_a)
            # )
            # return True, np.fromiter(
            #     (ls[1].item() for ls in -torch.log_softmax(
            #         self.model(**(
            #             self.tokenizer(batch_a, batch_b, return_tensors=&#34;pt&#34;, truncation=&#39;only_second&#39;, padding=True)
            #         )).logits, dim=1
            #     )), float, len(batch_a)
            # )
            return True, np.fromiter(
                (item[1] for item in (-torch.log_softmax(
                    self.model(**(
                        self.tokenizer(batch_a, batch_b, return_tensors=&#34;pt&#34;, truncation=&#39;only_second&#39;, padding=True)
                    )).logits, dim=1)
                ).detach().cpu().numpy()), float, len(batch_a)
            )
        except RuntimeError as ex:  # Cuda Out Of Memory
            self.__logger.warning(ex)
            return False, str(ex)

    def _log_prob_loop(self, batch_size: int, queries: list, size_reduce: int, texts: list):
        size = len(queries)
        count = 0
        results = []
        while count &lt; size:
            ok, res = self._log_prob_batch(
                queries[count:count + batch_size],
                texts[count:count + batch_size]
            )
            if not ok:
                if batch_size == 1:
                    self.__logger.error(&#39;Runtime error during computation: %s&#39;, res)
                    raise Exception()
                new_size = batch_size - size_reduce
                new_size = new_size if new_size &gt;= 1 else 1
                self.__logger.warning(&#39;Trying to reduce batch size from %d to %d&#39;, batch_size, new_size)
                batch_size = new_size
                gc.collect()  # force garbage collection
                continue
            results.append(res)
            count = count + batch_size
        CACHE[&#39;batch_size&#39;] = batch_size
        return np.concatenate(results, dtype=float)

    def log_prob_df(self, data: pd.DataFrame, inplace=False, batch_size: int = None, size_reduce=5) -&gt; pd.DataFrame:
        &#34;&#34;&#34;Compute the log-likelihood between queries and doc in a dataset.

        :param data: input df
        :param inplace: if modify data or create a copy
        :param batch_size: number of rows to compute at time
        :param size_reduce: how much reduce batch_size if computation fail for OOM
        :return: df with &#39;log_prob&#39; column&#34;&#34;&#34;
        if not inplace:
            data = data.copy()

        if batch_size is None:
            batch_size = CACHE[&#39;batch_size&#39;]

        queries = list(data[&#39;query&#39;])
        texts = list(data[&#39;text&#39;])
        log_col = self._log_prob_loop(batch_size, queries, size_reduce, texts)

        data[&#39;log_prob&#39;] = log_col
        return data</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="convSearchPython.ranking.bert.bert_pre_marco.DL4MBertClassifier" href="#convSearchPython.ranking.bert.bert_pre_marco.DL4MBertClassifier">DL4MBertClassifier</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="convSearchPython.ranking.bert.bert_pre_marco.GpuDL4MBertClassifier.log_prob"><code class="name flex">
<span>def <span class="ident">log_prob</span></span>(<span>self, query: str, doc: str) ‑> float</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the log-likelihood probability between a query and a document</p>
<p>:param query: the query text
:param doc: the document text
:return: a float in [0,1] representing the log-likelihood probability</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def log_prob(self, query: str, doc: str) -&gt; float:
    &#34;&#34;&#34;Compute the log-likelihood probability between a query and a document

    :param query: the query text
    :param doc: the document text
    :return: a float in [0,1] representing the log-likelihood probability&#34;&#34;&#34;
    encoding = self.tokenizer(query, doc, return_tensors=&#34;pt&#34;, truncation=&#39;only_second&#39;)
    logits = self.model(**encoding).logits
    sft = torch.softmax(logits, dim=1)
    lg = - torch.log(sft[0][1])
    return lg.item()</code></pre>
</details>
</dd>
<dt id="convSearchPython.ranking.bert.bert_pre_marco.GpuDL4MBertClassifier.log_prob_df"><code class="name flex">
<span>def <span class="ident">log_prob_df</span></span>(<span>self, data: pandas.core.frame.DataFrame, inplace=False, batch_size: int = None, size_reduce=5) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the log-likelihood between queries and doc in a dataset.</p>
<p>:param data: input df
:param inplace: if modify data or create a copy
:param batch_size: number of rows to compute at time
:param size_reduce: how much reduce batch_size if computation fail for OOM
:return: df with 'log_prob' column</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def log_prob_df(self, data: pd.DataFrame, inplace=False, batch_size: int = None, size_reduce=5) -&gt; pd.DataFrame:
    &#34;&#34;&#34;Compute the log-likelihood between queries and doc in a dataset.

    :param data: input df
    :param inplace: if modify data or create a copy
    :param batch_size: number of rows to compute at time
    :param size_reduce: how much reduce batch_size if computation fail for OOM
    :return: df with &#39;log_prob&#39; column&#34;&#34;&#34;
    if not inplace:
        data = data.copy()

    if batch_size is None:
        batch_size = CACHE[&#39;batch_size&#39;]

    queries = list(data[&#39;query&#39;])
    texts = list(data[&#39;text&#39;])
    log_col = self._log_prob_loop(batch_size, queries, size_reduce, texts)

    data[&#39;log_prob&#39;] = log_col
    return data</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="convSearchPython.ranking.bert" href="index.html">convSearchPython.ranking.bert</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="convSearchPython.ranking.bert.bert_pre_marco.DL4MBertClassifier" href="#convSearchPython.ranking.bert.bert_pre_marco.DL4MBertClassifier">DL4MBertClassifier</a></code></h4>
<ul class="">
<li><code><a title="convSearchPython.ranking.bert.bert_pre_marco.DL4MBertClassifier.log_prob_df" href="#convSearchPython.ranking.bert.bert_pre_marco.DL4MBertClassifier.log_prob_df">log_prob_df</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="convSearchPython.ranking.bert.bert_pre_marco.CpuDL4MBertClassifier" href="#convSearchPython.ranking.bert.bert_pre_marco.CpuDL4MBertClassifier">CpuDL4MBertClassifier</a></code></h4>
<ul class="">
<li><code><a title="convSearchPython.ranking.bert.bert_pre_marco.CpuDL4MBertClassifier.log_prob" href="#convSearchPython.ranking.bert.bert_pre_marco.CpuDL4MBertClassifier.log_prob">log_prob</a></code></li>
<li><code><a title="convSearchPython.ranking.bert.bert_pre_marco.CpuDL4MBertClassifier.log_prob_df" href="#convSearchPython.ranking.bert.bert_pre_marco.CpuDL4MBertClassifier.log_prob_df">log_prob_df</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="convSearchPython.ranking.bert.bert_pre_marco.GpuDL4MBertClassifier" href="#convSearchPython.ranking.bert.bert_pre_marco.GpuDL4MBertClassifier">GpuDL4MBertClassifier</a></code></h4>
<ul class="">
<li><code><a title="convSearchPython.ranking.bert.bert_pre_marco.GpuDL4MBertClassifier.log_prob" href="#convSearchPython.ranking.bert.bert_pre_marco.GpuDL4MBertClassifier.log_prob">log_prob</a></code></li>
<li><code><a title="convSearchPython.ranking.bert.bert_pre_marco.GpuDL4MBertClassifier.log_prob_df" href="#convSearchPython.ranking.bert.bert_pre_marco.GpuDL4MBertClassifier.log_prob_df">log_prob_df</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>